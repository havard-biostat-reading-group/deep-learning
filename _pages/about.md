---
permalink: /
title: Deep Learning Reading Group!
excerpt: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---



* Pick a paper every week to discuss the details and also may be some of its relevant references. One of us can lead the discussion but the hope is everyone will read the paper(s) and try to learn together.
* I will present for the first 2-3 weeks and then hopefully some other people will pick it up. For the first meeting on March 12, I will try to present an overview (from my limited knowledge) of the kinds of questions that typically arise in the deep learning literature. In particular, I will talk about the learning problem with feed forward neural nets (which will constitute a major proportion of the initial readings I plan to pursue) and fitting techniques (stochastic gradient descent and back-propagation). 
* I will start chronologically w.r.t. the development of deep learning since mid 1980's. More precisely, I first plan on focusing on approximation properties of feed forward neural nets (papers by Cybenko (1988) , (Hornik, Snitchcombe, & White (1989)), Barron (1992, 1993)). This can connect to a large body of recent literature (Yarotsky (2017,2018), Suzuki (2018), Petersen and Viogtlaender (2018), Telgarsky (2015), Safran and Shamir (2017)..) to understand the trade-off between depth and width in quantifying such approximation errors.
* Subsequently, we can briefly touch on connections of this approximation theory (as well as VC dimension of neural nets) with statistical learning theory and how recent literature have used in several statistical theory papers.
* Next topics can include (and we can decide the order): issues/benefits of overfitting and Generalization error bouds;   ; SGD and loss surface; GAN and connections with Robust Statistics; 
