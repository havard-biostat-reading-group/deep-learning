 “Deep Learning” Reading Group

 Tuesday’s 3:30-5 pm

 Room 426, Building II

 Harvard T.H. Chan School of Public Health

In this reading group, we plan to explore specific aspects of the
subject area which include (i) approximation and learning theory, (iii)
generalization error bounds, (iv) computational complexity and
overfitting issues, and (v) generative adversarial networks.

-   We will pick a few papers every week to discuss in detail. One of us
    can lead the discussion but the hope is everyone will read the
    paper(s) and try to learn together.

-   I will present for the first 2-3 weeks and then hopefully some other
    people will pick it up. For the first meeting on March 12, I will
    try to present an overview (from my limited knowledge) of the kinds
    of questions that typically arise in the deep learning literature.
    In particular, I will talk about the learning problem with feed
    forward neural nets (which will constitute a major proportion of the
    initial readings) and fitting techniques (stochastic gradient
    descent and back-propagation).

-   We will start chronologically w.r.t. the development of deep
    learning since mid 1980's. More precisely, we will start by focusing
    on approximation properties of feed forward neural nets (papers by
    Cybenko (1988) , (Hornik, Snitchcombe, & White (1989)), Barron
    (1992, 1993)). This will connect to a large body of recent
    literature (Yarotsky (2017,2018), Suzuki (2018), Petersen and
    Viogtlaender (2018), Telgarsky (2015), Safran and Shamir (2017) etc)
    to understand the trade-off between depth and width in quantifying
    such approximation errors.

-   Subsequently, we will briefly touch on connections of this
    approximation theory (as well as VC dimension of neural nets) with
    statistical learning theory.

-   Next topics will include (and we can decide the order):
    issues/benefits of overfitting and Generalization error bouds; ; SGD
    and loss surface; GAN and connections with Robust Statistics;


