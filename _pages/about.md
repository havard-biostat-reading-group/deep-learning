---
permalink: /
title: Deep Learning Reading Group!
excerpt: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Dear All

I am finally starting the deep learning reading group that I have meaning to for a while. The meetings will be from 3 - 4:30 pm in HSPH Building II, Room 426. The first meeting is March 12, 2019. I have also attached a brief description of the reading group for advertising.


I am initially reaching out to you as a small group who I have chatted with personally a few times about this reading group. Please let me know if there are any suggestions on content, structure, papers to be included etc. For now I am planning on proceeding along the following bullet points. Please feel free to send me comments on the plan and topics.

Best

Rajarshi

* Pick a paper every week to discuss the details and also may be some of its relevant references. One of us can lead the discussion but the hope is everyone will read the paper(s) and try to learn together.
* I will present for the first 2-3 weeks and then hopefully some other people will pick it up. For the first meeting on March 12, I will try to present an overview (from my limited knowledge) of the kinds of questions that typically arise in the deep learning literature. In particular, I will talk about the learning problem with feed forward neural nets (which will constitute a major proportion of the initial readings I plan to pursue) and fitting techniques (stochastic gradient descent and back-propagation). 
* I will start chronologically w.r.t. the development of deep learning since mid 1980's. More precisely, I first plan on focusing on approximation properties of feed forward neural nets (papers by Cybenko (1988) , (Hornik, Snitchcombe, & White (1989)), Barron (1992, 1993)). This can connect to a large body of recent literature (Yarotsky (2017,2018), Suzuki (2018), Petersen and Viogtlaender (2018), Telgarsky (2015), Safran and Shamir (2017)..) to understand the trade-off between depth and width in quantifying such approximation errors.
* Subsequently, we can briefly touch on connections of this approximation theory (as well as VC dimension of neural nets) with statistical learning theory and how recent literature have used in several statistical theory papers.
* Next topics can include (and we can decide the order): issues/benefits of overfitting and Generalization error bouds;   ; SGD and loss surface; GAN and connections with Robust Statistics; 
